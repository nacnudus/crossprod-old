{"pages":[{"url":"http://nacnudus.github.io/crossprod/creating-nests-without-tidyr","text":"Unless you begin with an unnested data frame, creating a nested data frame needs a little trick. Here it is. Nested data frames The tidyr package has a handy function for nesting data frames. Hadley Wickham describes it thus : In a grouped data frame, you have one row per observation, and additional metadata define the groups. In a nested data frame, you have one row per group, and the individual observations are stored in a column that is a list of data frames. This is a useful structure when you have lists of other objects (like models) with one element per group. Here's a small example: library ( dplyr ) library ( tidyr ) iris_nested <- iris %>% group_by ( Species ) %>% sample_n ( 2 ) %>% nest iris_nested ## Source: local data frame [3 x 2] ## ## Species data ## <fctr> <list> ## 1 setosa <tbl_df [2,4]> ## 2 versicolor <tbl_df [2,4]> ## 3 virginica <tbl_df [2,4]> iris_nested %>% str ## Classes 'tbl_df', 'tbl' and 'data.frame': 3 obs. of 2 variables: ## $ Species: Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 2 3 ## $ data :List of 3 ## ..$ :Classes 'tbl_df', 'tbl' and 'data.frame': 2 obs. of 4 variables: ## .. ..$ Sepal.Length: num 5.1 5.1 ## .. ..$ Sepal.Width : num 3.7 3.8 ## .. ..$ Petal.Length: num 1.5 1.6 ## .. ..$ Petal.Width : num 0.4 0.2 ## ..$ :Classes 'tbl_df', 'tbl' and 'data.frame': 2 obs. of 4 variables: ## .. ..$ Sepal.Length: num 5.6 6.5 ## .. ..$ Sepal.Width : num 2.5 2.8 ## .. ..$ Petal.Length: num 3.9 4.6 ## .. ..$ Petal.Width : num 1.1 1.5 ## ..$ :Classes 'tbl_df', 'tbl' and 'data.frame': 2 obs. of 4 variables: ## .. ..$ Sepal.Length: num 7.7 6.3 ## .. ..$ Sepal.Width : num 2.6 2.9 ## .. ..$ Petal.Length: num 6.9 5.6 ## .. ..$ Petal.Width : num 2.3 1.8 Interestingly, the nested column isn't a vector like ordinary columns; it's a list. Actually lists are just one kind of vector — the non-atomic kind (composed of parts, i.e vectors and other lists), whereas integer/character/etc. vectors are the atomic kind (not composed of parts). This is nicely explained in Advanced R by Hadley Wickham. is.atomic ( vector ( mode = \"character\" , length = 2 )) ## [1] TRUE is.atomic ( vector ( mode = \"list\" , length = 2 )) ## [1] FALSE Please say it's a data frame Data frames, which are a list of vectors, handle list-type columns perfectly well, but data-frame-construction functions don't. So when I tried to create one from scratch (rather than by nesting an existing data frame as above), I lost a lot of time mucking about with data.frame() and the like. data.frame ( X1 = 1 : 2 , X2 = list ( iris , mtcars )) ## Error in ( function (..., row . names = NULL , check . rows = FALSE , check . names = TRUE , : arguments imply differing number of rows : 150 , 32 as.data.frame ( list ( X1 = 1 : 2 , X2 = list ( iris , mtcars ))) ## Error in ( function (..., row . names = NULL , check . rows = FALSE , check . names = TRUE , : arguments imply differing number of rows : 150 , 32 It's a data frame because I say so The answer is to simply tell R that the data structure is a data frame by setting its class and giving it a \"row.names\" attribute. x <- list ( X1 = 1 : 2 , X2 = list ( iris [ 1 : 2 , 1 : 2 ], iris [ 3 : 5 , 1 : 4 ])) structure ( x , class = c ( \"tbl_df\" , \"data.frame\" ), row.names = 1 : 2 ) ## Source: local data frame [2 x 2] ## ## X1 X2 ## <int> <list> ## 1 1 <data.frame [2,2]> ## 2 2 <data.frame [3,4]> Invading the nest Accessing the nested column by the usual subsetting operators, $ , [ and [[ , is a little clumsy. x $ X2 # Returns the list of data frames ## [[1]] ## Sepal.Length Sepal.Width ## 1 5.1 3.5 ## 2 4.9 3.0 ## ## [[2]] ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 x $ X2 [ 2 ] # Returns the second data frame, wrapped in a list ## [[1]] ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 x $ X2 [[ 2 ]] # Returns the second data frame -- probably what you want ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 x [[ \"X2\" ]][[ 2 ]] # Returns the second data frame -- also probably what you want ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 x [ 2 , \"X2\" ] # Returns the second data frame wrapped in another data frame ## Error in x[2, \"X2\"]: incorrect number of dimensions x [ 2 , \"X2\" , drop = TRUE ] # Same -- ignores \"drop\" ## Error in x[2, \"X2\", drop = TRUE]: incorrect number of dimensions x [ 2 , \"X2\" ][ 1 , \"X2\" ][ 1 , \"X2\" ] # The `[` goes around in circles ## Error in x[2, \"X2\"]: incorrect number of dimensions","tags":"Miscellaneous","title":"Creating nests without tidyr"},{"url":"http://nacnudus.github.io/crossprod/brexit-poll-of-polls","text":"This post does the following: Re-works the Financial Times poll-of-polls graph Explores the relationship between sample size, polling method, and voting intention. Data I scraped the poll data from the Financial Times poll of polls . The Financial Times made this graph of it: To check a later inference about sample sizes and online/telephone methods, I also scraped polling data from the BBC poll of polls and used it to augment the Financial Times data with the polling method. My analysis focusses on the Financial Times data, because the sample sizes are provided, there is a longer time-series, and I didn't notice the BBC 's version until I'd done most of the work. Reworking the graph The Financial Times graph emphasises the poll-of-polls statistic, and the difference between online and telephone polls. In my version, I want to emphasise the outcomes (the majority in each poll), the margins of the majorities, and the sample sizes. I also present the full series. I would have included the poll-of-polls statistic on my graph, since the Financial Times describes their method in a footnote: The FT poll of polls is calculated by taking the last seven polls from unique pollsters up to a given date, removing the two polls with the highest and lowest shares for ‘remain', and calculating an adjusted average of the five remaining polls, where the more recent polls are given a higher weight Unfortunately, besides omitting the weights, and their tie-breaking policy, their statistic has obviously been redesigned since the footnote was written, because their current statistic for ‘remain' is higher than the second-highest ‘remain' result in the last seven polls. Here's that graph again, but this time beginning in September 2015 like the Financial Times. Something worth noticing is that the ‘leave' majorities are mostly large samples. Comparing this graph with the one by the Financial Times, sample size seems to be a proxy for telephone (small) vs online (large) polling methods. Let's check. Although the Financial Times graph distinguishes between online/telephone methods, that information isn't included in the table, despite its obvious importance : There's a big difference between the online and telephone polls on the EU referendum – with online polls showing the sides neck-and neck and telephone polls showing about a 15% gap in favour of ‘remain'. Why? Fortunately, for most polls in the last six months, data from the BBC 's poll of polls can augment the Financial Times data with online/telephone information. As the following frequency table shows, in nearly all matched polls, large samples correspond with an online method. So while large samples appear to favour ‘leave', it may simply be that online polls do. ## ## online phone <NA> Sum ## < 1400 8 18 35 61 ## ≥ 1400 67 0 114 181 ## <NA> 10 5 0 15 ## Sum 85 23 149 257 However, telephone polls do not necessarily favour either side. Comparing the frequencies of outcomes with first methods and then sample sizes, the association between small sample sizes and a ‘remain' outcome appears to be much stronger than between ‘online' and ‘remain' or ‘phone' and ‘remain'. Perhaps this is why the financial markets apparently regard telephone polls as more reliable, despite the smaller sample sizes. ## ## leave remain <NA> Sum ## online 26 49 10 85 ## phone 1 17 5 23 ## <NA> 52 97 0 149 ## Sum 79 163 15 257 ## ## leave remain <NA> Sum ## < 1400 15 46 0 61 ## ≥ 1400 64 117 0 181 ## <NA> 0 0 15 15 ## Sum 79 163 15 257 Outcome by sample size / polling method I already noted that ‘leave' majorities tend to come from large-sample/online polls. The next graph makes this more obvious. Justification of large/small threshold But how did I choose 1400 as the boundary between small and large samples? It's because of the following visualisations, Polls with samples smaller than 1400 just seem to behave differently. Perhaps small samples don't find the ‘leave' voters, or perhaps they do find the ‘remain' ones. Smaller samples also don't find the undecided people (this is not quite as convincing as the graph above). Checking this against the method data from the BBC , I'm arguably on the right track. It would obviously be best to know the method as well as the sample size, but since I'm using the Financial Times data, and since I don't have the method of so many of those polls (grey points below), I have focussed on sample size instead. Indecision favours a ‘leave' outcome: Part I — graph Here I can use stats, the only stats I've ever been taught (the really out-of-date stuff), to explore whether undecided voters will favour the status quo. (What is my status quo, anyway — that we're in Europe now, or that I've always wanted to leave?) Here's the association between indecision and the ‘remain' vote. Intermission (obvious glitch) A few ‘remain' majorities are below the ‘win' threshold in the graph above (green points below the dotted line). That could be because of missing \"won't vote\" information. See YouGov's explanation : Telephone polls ask their respondents \"How will you vote in the referendum?\" People are assumed to have an opinion, and 90% of them give one. By contrast, online polls present people with options: remain, leave, won't vote, don't know – there is less assumption of an opinion, and 20% or more don't offer one. A few polls total much less than 100%, probably for the same reason, but it isn't a problem in most cases. Indecision favours a ‘leave' outcome: Part II — stats We've already seen the non-linearity of sample size vs everything, so I build two models, first for large samples, then for small ones. ## ## Call: ## lm(formula = remain ~ undecided, data = master_ft %>% filter(sample_size == ## \"≥ 1400\")) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.152068 -0.015749 0.004864 0.024251 0.100785 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 0.49431 0.01027 48.130 < 2e-16 *** ## undecided -0.46932 0.05599 -8.383 1.49e-14 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.03745 on 179 degrees of freedom ## Multiple R-squared: 0.2819, Adjusted R-squared: 0.2779 ## F-statistic: 70.27 on 1 and 179 DF, p-value: 1.487e-14 The coefficient of ‘undecided' is nearly -0.5, suggesting that undecided large-sample voters about as likely to vote either way (lines almost parallel in the graph below). But as proportion of undecided voters reduces, at what point does the ‘remain' outcome start to benefit? (this analysis will be more meaningful for small samples, in just a moment). There are two linear functions: the fitted model, and the threshold of a majority (depending on the proportion of voters who are undecided). Not only can we plot these functions (and base R is simplest here), but we can solve them for the fulcrum, which turns out to be about 19%. If the proportion of voters who are undecided is below 19%, then outcome is likely to be ‘remain'. I exhibit the R code here, for anyone interested in plotting functions and solving them. remain <- function ( x ) { coef ( lm_large )[ 2 ] * x + coef ( lm_large )[ 1 ]} majority <- function ( x ) {( 1 - x ) / 2 } plot ( remain , 0 , 0.5 , xlab = \"undecided\" , col = \"blue\" ) plot ( majority , 0 , 0.5 , col = \"brown\" , add = TRUE ) fulcrum <- function ( x ) { remain ( x ) - majority ( x )} uniroot ( fulcrum , interval = c ( 0 , 1 )) $ root ## [1] 0.1855785 master_ft %>% filter ( sample_size == \"≥ 1400\" , undecided >= 0.1855785 ) %>% nrow ## [1] 89 Since \"small\" may be a proxy for \"online\", let's model that, too. ## ## Call: ## lm(formula = remain ~ undecided, data = master_ft %>% filter(sample_size == ## \"< 1400\")) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.147178 -0.028871 0.004515 0.035027 0.178669 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 0.57795 0.01514 38.177 < 2e-16 *** ## undecided -0.80511 0.09922 -8.114 3.51e-11 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.05248 on 59 degrees of freedom ## Multiple R-squared: 0.5274, Adjusted R-squared: 0.5194 ## F-statistic: 65.84 on 1 and 59 DF, p-value: 3.507e-11 This time, the coefficient of undecided is about -0.8, suggesting that undecided small-sample voters are more likely to vote to remain. The fulrum, now much more meaningful than above, given the coefficient, is at about 26%, with a caveat that there are only five observations above 26%. remain <- function ( x ) { coef ( lm_small )[ 2 ] * x + coef ( lm_small )[ 1 ]} majority <- function ( x ) {( 1 - x ) / 2 } plot ( remain , 0 , 0.5 , xlab = \"undecided\" , col = \"blue\" ) plot ( majority , 0 , 0.5 , col = \"brown\" , add = TRUE ) fulcrum <- function ( x ) { remain ( x ) - majority ( x )} uniroot ( fulcrum , interval = c ( 0 , 1 )) $ root ## [1] 0.2554619 master_ft %>% filter ( sample_size == \"< 1400\" , undecided >= 0.255461 ) %>% nrow ## [1] 5 Zero undecided voters Finally, what about the five zero-undecided polls (the five points along the bottom of the timeseries)? It turns out that those polls were all conducted by the ORB company, and they're also the large-sample polls by ORB . They aren't included in the BBC data, so we can't tell whether or not they are online polls. Make of them what you will. ## Source: local data frame [5 x 11] ## ## remain leave undecided date pollster sample sample_size majority ## <dbl> <dbl> <dbl> <date> <chr> <dbl> <chr> <chr> ## 1 0.50 0.50 0 2016-04-29 ORB 2000 ≥ 1400 remain ## 2 0.51 0.49 0 2016-03-28 ORB 2002 ≥ 1400 remain ## 3 0.48 0.52 0 2015-11-19 ORB 2067 ≥ 1400 leave ## 4 0.53 0.47 0 2015-10-25 ORB 2015 ≥ 1400 remain ## 5 0.55 0.45 0 2015-09-06 ORB 2044 ≥ 1400 remain ## Variables not shown: max_percent <dbl>, min_percent <dbl>, method <chr>.","tags":"Miscellaneous","title":"Brexit poll of polls"},{"url":"http://nacnudus.github.io/crossprod/simmer-vs-simpy-the-bank-part-ii","text":"Simmer vs SimPy (rematch) In my previous post , I ported Part I of SimPy ‘ s flagship tutorial, The Bank , to simmer . This post does the same for Part II , which introduces tricky concepts for tricky customers. This post does two things: Discusses some difficulties implementing The Bank: Part II . Suggests some reasons why simulation is hard (blame humans). The Bank ‘ The Bank' is a tutorial that develops DES concepts and techniques by simulating the paths of customers at a bank. The arrivals (customers) queue for a server (counter), are served, and exit. Complete code The actual ported code is available on GitHub , and I only give simple examples in this post. Priority and pre-emption High-priority arrivals (customers) go straight to the front of the queue. When pre-emption is allowed, they can even barge they way onto a busy server, interrupting an arrival (customer) that is already being served. Priority and pre-emption has only recently been added to simmer , and it still feels a bit clumsy. The complications arise because events have to be rescheduled, and decisions have to be remade. For example: When a server is serving several arrivals at once, which one should be interrupted by a higher-priority arrival? Simmer already implements first-in-first-out ( FIFO ) and last-in-first-out ( LIFO ) policies. When an arrival is interrupted while being served, and then resumes service, should they start again (repeating the first period of their service time) or carry on from where they left off (completing their remaining service time). Simmer already implements these two options, but it's reasonable to suppose that resumption of service might come with a time penalty, and it doesn't currently seem to be possible to express such a penalty. While an interrupted arrival is waiting to return to the server, where do they wait? This matters in a finite queue. Currently, simmer allows the arrival to wait in the finite queue, even if the queue is already full and rejecting new arrivals. To change this behaviour, one would have to define a policy for ejecting arrivals from the queue to maintain the constraint on its size. Balking and reneging Balking is the behaviour of an arrival that, for some reason, never enters a queue. The example in The Bank Tutorial is the case when a finite queue is full, so the arrival is rejected. An alternative might be that the arrival decides whether or not to enter the queue according to the number of arrivals already in it. That scenario can already be implemented in simmer by branching based on an enquiry into the state of the queue. Reneging is the behaviour of an arrival already in a queue, who decides to leave it. This isn't yet possible in simmer , but the authors have indulged me in several discussions about it, on GitHub and in the discussion forum . The difficulty, as I currently see it, is that the seize function, by handling the whole interval between entering the queue and reaching the server, makes the queueing period relatively inaccessible. If one wanted an arrival to renege from one queue and branch instead into another queue, there is no way to express that inside the seize function. If one wanted other customers in the queue to reassess their patience, based on customers ahead of them reneging, then there is no way to express that idea either. To be fair to the simmer authors, these ideas weren't present in early versions of SimPy either, and were still clumsy when The Bank: Part II was written. It also can't be easy to separate the concept of queueing from the seize function, since this is probably the most computationally-expensive aspect of modelling, which has, very sensibly, been implemented in C++. Interruption SimPy provides functions for interrupting an arrival that is being served. The examples in The Bank Tutorial don't convince me that special functions are necessary. Why not simply increase their service time? That's certainly how I implemented interruptions in simmer . Wait until Another relatively new feature of simmer is the ability to schedule the capacity of resources at certain times. I used schedules to implement the bank opening in the morning, and to ‘open the door' once every 30 minutes to let in any customers that are queueing outside. The scheduling feature can be periodic, which is wonderful, but it isn't currently possible to schedule a single change in capacity that then endures indefinitely. See GitHub for examples. It also doesn't seem to be possible to schedule infinite capacity. [ EDIT ] I was wrong. It is possible to do both those things. Monitoring and plotting Here is where simmer continues to excel, providing far simpler and more-intuitive monitoring of arrivals, resources and attributes, in handy data frames for straightforward plotting with any graphics library. Simulation is hard Because human behaviour is hard. Real-life systems involving humans are massively parallel. Every actor processes his/her own activities onto the universal time-line. As long as computers have very finite numbers of processors, simulation libraries will have to find ways to express this parallelism in a way that computers can serialise. When actors in a system influence each-other's behaviour, the computational difficulties of serialising their behaviour begin to meet the boundaries of efficient computation. The authors of simmer have a very generous attitude towards suggestions and discussion, like so many R developers. No doubt that this post will soon become obsolete by their efforts.","tags":"Miscellaneous","title":"Simmer vs SimPy: The Bank, Part II"},{"url":"http://nacnudus.github.io/crossprod/simmer-vs-simpy-the-bank-part-i","text":"Simmer vs SimPy Which package would be easier for teaching queueing theory? Python 2.7's SimPy , designed for (as far as I can tell) lecturing, by Tony Vigneau at my alma mater, Vic Uni Wellington NZ , or simmer , designed by Bart Smeets and Iñaki Ucar to (as far as I can tell) actually use? The simmer package is a relatively new R package for discrete event simulation ( DES ). It's an exciting development, because there isn't a lot of open-source DES software. SimPy seems to be the only serious competitor for teaching DES and queueing theory. This post does three things: Ports the code of the main SimPy tutorial ‘The Bank' to simmer . Opines that simmer would be easier to teach as part of a queueing theory course. Pursues a random red herring. Why not SimPy 3? I use SimPy 2 (for Python 2), because it is the last version developed by the original author, because it was the version I was taught, only last year, and because, in one crucial respect (monitoring), it's much easier to use . The Bank ‘ The Bank' is a tutorial that develops DES concepts and techniques by simulating the paths of customers at a bank. The arrivals (customers) queue for a server (counter), are served, and exit. Complete example The actual ported code is available on GitHub , and I only give simple examples in this post. The first example is complete. First, SimPy : \"\"\" bank01: The single non-random Customer \"\"\" from SimPy.Simulation import * ## Model components ----------------------------- class Customer ( Process ): \"\"\" Customer arrives, looks around and leaves \"\"\" def visit ( self , timeInBank ): print now (), self . name , \" Here I am\" yield hold , self , timeInBank print now (), self . name , \" I must leave\" ## Experiment data ------------------------------ maxTime = 100.0 # minutes timeInBank = 10.0 # minutes ## Model/Experiment ------------------------------ initialize () c = Customer ( name = \"Klaus\" ) activate ( c , c . visit ( timeInBank ), at = 5.0 ) simulate ( until = maxTime ) ## 5.0 Klaus Here I am ## 15.0 Klaus I must leave Next, simmer : # bank01: The single non-random customer suppressMessages ( library ( simmer )) ## Experiment data ------------------------------ maxTime <- 100 # minutes timeInBank <- 10 # minutes ## Model components ----------------------------- customer <- create_trajectory ( \"Customer's path\" ) %>% timeout ( function () { timeInBank }) ## Model/Experiment ------------------------------ bank <- simmer ( \"bank\" ) bank %>% add_generator ( \"Customer\" , customer , at ( 5 )) ## simmer environment: bank | now: 0 | next: 5 ## { Generator: Customer | monitored: 1 | n_generated: 1 } bank %>% run ( until = maxTime ) ## simmer environment: bank | now: 15 | next: ## { Generator: Customer | monitored: 1 | n_generated: 1 } bank %>% get_mon_arrivals ## name start_time end_time activity_time finished replication ## 1 Customer0 5 15 10 TRUE 1 Already there are several differences that might make teaching queueing theory with simmer easier than with SimPy : The difference between from X import Y and Import X isn't relevant. Whitespace doesn't matter. The difference between integer and floating-point types doesn't matter here. Arguments don't have to be defined. References don't have to be passed. self is irrelevant. timeout is more intuitive than yield ( yield describes how the class behaves in the implementation of the DES , as it yields control back to the clock, whereas timeout describes what the function does in the mind of the modeller). But there is one point that could be tricky, and that soon becomes important: timeout and add_generator both expect functions , rather than vectors, to control (inter-)arrival time and timeout duration. It would be nice to have syntactic sugar to handle vectors. The reason for the functions is that, when a model is run indefinitely, a function can continue generating new arrival times and timeout durations, whereas a vector will soon be exhausted. Example fragments Implementing the rest of the examples brought up a few other interesting points. Generate more than one arrival In the SimPy examples, to generate n > 1 arrivals, the activate code to generate them moves inside the Source class. To explain why requires a quite a lot of understanding/intuition of object-oriented programming that isn't relevant to learning about queuing theory. Simmer doesn't present this difficulty. Limit the number of arrivals Arrivals with random inter-arrival times would be generated indefinitely by bank %>% add_generator(\"Customer\", customer, function() {runif(1)}) . To limit this to n = 10 arrivals, you might try times <- runif(10); bank %>% add_generator(\"Customer\", customer, times) , but it doesn't work, because add_generator expects a function that will supply inter-arrival times, not a vector that does supply them. Simmer provides a handy function, at() , to convert a vector to function, so you could do add_generator(\"Customer\", customer, at(runif(10))) , except that this still doesn't work. That's because at() is designed to convert arrival times into inter-arrival times, but the runif function is being used to provide inter-arrival times in the first place. The final fix is to do add_generator(\"Customer\", customer, at(c(0, cumsum(runif(10))))) . Joining the shortest queue This is a pain in both SimPy and simmer . The SimPy example creates a method to return the length of each queue, and then the following code iterates through the results until a queue is chosen: # Select the shortest queue for i in range ( Nc ): if Qlength [ i ] == 0 or Qlength [ i ] == min ( Qlength ): choice = i # the chosen queue number break # Join the queue yield request , self , counters [ choice ] In simmer , this is done by branching: customer <- create_trajectory ( \"Customer's path\" ) %>% branch ( function () { # Select the shortest queue which.min ( c ( bank %>% get_server_count ( \"counter1\" ) + bank %>% get_queue_count ( \"counter1\" ), bank %>% get_server_count ( \"counter2\" ) + bank %>% get_queue_count ( \"counter2\" ))) }, merge = rep ( TRUE , 2 ), # Join the first queue, if it was chosen create_trajectory ( \"branch1\" ) %>% seize ( \"counter1\" ) %>% timeout ( function () { rexp ( 1 , 1 / timeInBank )}) %>% release ( \"counter1\" ), # Otherwise join the second queue, if it was chosen create_trajectory ( \"branch2\" ) %>% seize ( \"counter2\" ) %>% timeout ( function () { rexp ( 1 , 1 / timeInBank )}) %>% release ( \"counter2\" )) I mucked about for a while trying to avoid branching by using attributes to name the server at seize time. I won't explain attributes here because they're covered in the excellent simmer vignettes , but basically the following code doesn't work because attributes are only available to certain arguments, the resource argument not among them, only amount and perhaps priority and preemptible . # This doesn't work: customer <- create_trajectory ( \"Customer's path\" ) %>% # Attributes can be set, to choose the queue set_attribute ( \"counter\" , function () { which.min ( c ( bank %>% get_server_count ( \"counter1\" ) + bank %>% get_queue_count ( \"counter1\" ), bank %>% get_server_count ( \"counter2\" ) + bank %>% get_queue_count ( \"counter2\" )))}) %>% # But they aren't available in the `resource` argument of `seize` for naming # the server, so this doesn't work. seize ( function ( attrs ) { paste0 ( \"counter\" , attrs [ \"counter\" ])}) %>% timeout ( function () { rexp ( 1 , 1 / timeInBank )}) %>% release ( function ( attrs ) { paste0 ( \"counter\" , attrs [ \"counter\" ])}) Monitoring Simmer has a killer feature: everything is monitored automatically, and reported in handy data frames. This works especially well when doing many replications. But it isn't obvious how to do the equivalent of, in Python, injecting print or cat commands to describe the state of particular arrivals and servers. Presumably something could be done in the functions passed to dist arguments. In this sence, simmer is more declarative; like a story book, where the text describes the characters, but the characters don't really exist. Simmer describes arrivals and servers, but they don't really exist, and can't be directly interacted with. Random red herring Python 2.7, R and MATLAB all use the Mersenne-Twister algorithm by default. But none of them matches. The numpy Python package does match MATLAB (except for seed = 0), but not R. Two potential solutions are: Generate any old random numbers, write them to disk, and read them into both Python and R. Use rpy2 to use R's random number generator from within Python. I used rpy2 , but it wasn't long before I encountered a more serious problem. When random draws are conducted in more than one part of the code, the programmer can't control the order of the draws. That's up to SimPy and simmer . At that point, I gave up.","tags":"Miscellaneous","title":"Simmer vs SimPy: The Bank, Part I"},{"url":"http://nacnudus.github.io/crossprod/new-r-package-nzcrash","text":"Introducing the nzcrash package This package redistributes crash statistics already available from the New Zealand Transport Agency, but in a more convenient form. It's a large package (over 20 megabytes, compressed). library (nzcrash) library (dplyr) library (tidyr) library (magrittr) library (stringr) library (ggplot2) library (scales) library (lubridate) Datasets The crashes dataset describes most facts about a crash. The datasets causes , vehicles , and objects_struck describe facts that are in a many-to-one relationship with crashes. They can be joined to the crashes dataset by the common id column. The causes dataset can additionally be joined to the vehicles dataset by the combination of the id and vehicle_id columns. This is most useful when the resulting table is also joined to the crashes dataset. Up-to-date-ness The data was last scraped from the NZTA website on 2015-07-20. At that time, the NZTA had published data up to the 2015-03-10. dim (crashes) ## [1] 540888 32 dim (causes) ## [1] 888072 7 dim (vehicles) ## [1] 979930 3 dim (objects_struck) ## [1] 261276 3 Accuracy The NZTA , doesn't agree with itself about recent annual road tolls, and this dataset gives a third opinion. crashes %>% filter (severity == \"fatal\" ) %>% group_by ( year = year (date)) %>% summarize ( fatalities = sum (fatalities)) ## Source: local data frame [16 x 2] ## ## year fatalities ## 1 2000 462 ## 2 2001 455 ## 3 2002 405 ## 4 2003 461 ## 5 2004 435 ## 6 2005 405 ## 7 2006 393 ## 8 2007 421 ## 9 2008 366 ## 10 2009 384 ## 11 2010 375 ## 12 2011 284 ## 13 2012 308 ## 14 2013 256 ## 15 2014 279 ## 16 2015 34 Severity Crashes categorised as \"fatal\", \"serious\", \"minor\" or \"non-injury\", based on the casualties. If there are any fatalities, then the crash is a \"fatal\" crash, otherwise if there are any ‘severe' injuries, the crash is a \"serious\" crash. The definition of a ‘severe' injury is not clear. Minor and non-injury crashes are likely to be under-recorded since they often do not involve the police, who write most of the crash reports upon which these datasets are based. A common mistake is to confuse the number of fatal crashes with the number of fatalities. crashes %>% filter (severity == \"fatal\" ) %>% nrow ## [1] 5042 sum (crashes$fatalities) ## [1] 5723 Dates and times Three columns of the crashes dataset describe the date and time of the crash in the NZST time zone (Pacific/Auckland). date gives the date without the time time gives the time where this is available, and NA otherwise. Times are stored as date-times on the first of January, 1970. datetime gives the date and time in one value when both are available, and NA otherwise. date is always available, however time is not. When aggregating by some function of the date, e.g. by year, then always start from the date column unless you also need the time. This ensures against accidentally discounting crashes where a time is not recorded. crashes %>% filter ( is.na (time)) %>% count ( year = year (date)) %>% ggplot ( aes (year, n)) + geom_line () + ggtitle ( \"Crashes missing \\n time-of-day information\" ) crashes %>% filter ( is.na (time)) %>% count ( year = year (date)) %>% mutate ( percent = n/ sum (n)) %>% ggplot ( aes (year, percent)) + geom_line () + scale_y_continuous ( labels = percent) + ggtitle ( \"Percent of crashes missing \\n time-of-day information\" ) Location coordinates 99.9% of crashes have coordinates. These have been converted from the NZTM projection to the WGS84 projection for convenience with packages like ggmap . Because New Zealand is tall and skinny, you can easily spot the main population centres with a simple boxplot. crashes %>% ggplot ( aes (northing)) + geom_histogram ( binwidth = . 1 ) Vehicles There can be many vehicles in one crash, so vehicles are recorded in a separate vehicles dataset that can be joined to crashes by the common id column. crashes %>% inner_join (vehicles, by = \"id\" ) %>% count (vehicle) %>% arrange ( desc (n)) ## Source: local data frame [12 x 2] ## ## vehicle n ## 1 Car 728119 ## 2 Van, ute 87927 ## 3 SUV or 4x4 vehicle 48269 ## 4 Truck 44305 ## 5 Motorcycle 17733 ## 6 NA 16996 ## 7 Bicycle 15713 ## 8 Bus 8066 ## 9 Taxi or taxi van 6792 ## 10 Moped 3594 ## 11 Other or unknown 2043 ## 12 School bus 373 Objects struck There can be many objects struck in one crash, so these are recorded in a separate objects_struck dataset that can be joined to crashes by the common id column. Q: What are more fatal, trees or lamp posts? crashes %>% inner_join (objects_struck, by = \"id\" ) %>% filter (object %in% c ( \"Trees, shrubbery of a substantial nature\" , \"Utility pole, includes lighting columns\" ) , severity != \"non-injury\" ) %>% # non-injury crashes are poorly recorded count (object, severity) %>% group_by (object) %>% mutate ( percent = n/ sum (n)) %>% select (-n) %>% spread (severity, percent) ## Source: local data frame [2 x 4] ## ## object fatal serious minor ## 1 Utility pole, includes lighting columns 0.04432701 0.2149482 0.7407248 ## 2 Trees, shrubbery of a substantial nature 0.06742092 0.2459016 0.6866774 A: Trees (Don't worry, I know it's harder than that.) Causes Causes can be joined either to the crashes dataset (by the common id column), or to the vehicles dataset (by both of the commont id and vehicle_id ) columns. The main cause groups are given in the causes_category column. crashes %>% inner_join (causes, by = \"id\" ) %>% group_by (cause_category, id) %>% tally %>% group_by (cause_category) %>% summarize ( n = n ()) %>% arrange ( desc (n)) %>% mutate ( cause_category = factor (cause_category, levels = cause_category)) %>% ggplot ( aes (cause_category, n)) + geom_bar ( stat = \"identity\" ) + theme ( axis.text.x = element_text ( angle = 90 , hjust = 1 , vjust = . 5 )) That's odd — where are speed, alcohol, and restraints? They're given in cause_subcategory . causes %>% filter (cause_subcategory == \"Too fast for conditions\" ) %>% count (cause) %>% arrange ( desc (n)) ## Source: local data frame [8 x 2] ## ## cause n ## 1 Cornering 37861 ## 2 On straight 10196 ## 3 NA 7119 ## 4 To give way at intersection 1658 ## 5 At temporary speed limit 1010 ## 6 At crash or emergency 55 ## 7 Approaching railway crossing 44 ## 8 When passing stationary school bus 37 There's nothing there about speed limit violations, because it's impossible to tell what speed a vehicle was going at when it crashed. More worryingly, how is \"Alcohol test below limit\" a cause for a crash? Hopefully they filter those out when making policy decisions. levels (causes$cause) <- # Wrap facet labels str_wrap ( levels (causes$cause), 13 ) crashes %>% inner_join (causes, by = \"id\" ) %>% filter (cause_subcategory %in% c ( \"Alcohol or drugs\" )) %>% group_by (cause, id) %>% tally %>% group_by (cause) %>% summarize ( n = n ()) %>% # This extra step deals with many causes per crash arrange ( desc (n)) %>% mutate ( cause= factor (cause, levels = cause)) %>% ggplot ( aes (cause, n)) + geom_bar ( stat = \"identity\" ) + theme ( axis.text.x = element_text ( angle = 90 , hjust = 1 , vjust = . 5 )) rm (causes) # Because we messed up the factor levels This time, join causes to both vehicles and crashes to assess the drunken cyclist menace. crashes %>% filter (severity == \"fatal\" ) %>% select (id) %>% inner_join (vehicles, by = \"id\" ) %>% filter (vehicle == \"Bicycle\" ) %>% inner_join (causes, by = c ( \"id\" , \"vehicle_id\" )) %>% count (cause) %>% arrange ( desc (n)) ## Source: local data frame [55 x 2] ## ## cause n ## 1 Behind when changing lanes position or direction (includes U-turns) 26 ## 2 NA 20 ## 3 When required to give way to traffic from another direction 10 ## 4 Wandering or wobbling 8 ## 5 At Give Way sign 4 ## 6 Cyclist or M/cyclist wearing dark clothing 4 ## 7 Driving or riding on footpath 4 ## 8 On left without due care 4 ## 9 When pulling out or moving to the right 4 ## 10 At steady red light 3 ## .. ... .. I think we all know what \"Wandering or wobbling\" means. Check out the package on Github at https://github.com/nacnudus/nzcrash.","tags":"Miscellaneous","title":"New R package: nzcrash"}]}